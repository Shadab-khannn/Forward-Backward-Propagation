{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4410804-9dd9-4970-9a3f-d85afd916a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the purpose of forward propagation in a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb5281-784e-443e-8d29-db7ce4d15833",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec493fcb-a461-435e-9bd9-e5f0399c2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Forward propagation is the process of passing the input data through the layers of a neural network and computing the output.\n",
    "\n",
    "-The purpose of forward propagation is to make predictions based on the input data and the current state of the network parameters \n",
    "(weights and biases).\n",
    "\n",
    "-Forward propagation involves multiplying the input by the weights, adding the biases, and applying an activation function at each \n",
    "layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f0b09-2cab-477b-be43-570c58c867e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d1a669-9cc8-42e4-8e64-4651391d5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f1e6c-23d5-419f-af17-4c802454eef4",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00ee64-4ea9-45e8-8ddb-c3dcfeaa1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Single-layer feedforward neural network\n",
    "\n",
    "-A single-layer feedforward neural network consists of an input layer and an output layer, with no hidden layers in between.\n",
    "-The input layer has one neuron for each feature or variable in the input data, and the output layer has one neuron for each class or \n",
    "category in the output data.\n",
    "-The network learns a linear function that maps the input to the output, and applies an activation function to the output to introduce \n",
    "non-linearity and normalize the output values.\n",
    "\n",
    "Forward propagation\n",
    "\n",
    "-Forward propagation is the process of passing the input data through the network and computing the output.\n",
    "-The input data is represented as a vector x of size n×1, where n is the number of features or variables in the input data.\n",
    "-The output data is represented as a vector y of size m×1, where m is the number of classes or categories in the output data.\n",
    "-The network has two sets of parameters: a weight matrix W of size m×n, and a bias vector b of size m×1.\n",
    "-The weight matrix W contains the weights that determine the strength of the connection between each input neuron and each output neuron.\n",
    "-The bias vector b contains the biases that determine the threshold at which each output neuron is activated.\n",
    "-The network also has an activation function f that is applied element-wise to the output vector y.\n",
    "-The activation function f can be any non-linear function that maps real numbers to real numbers, such as sigmoid, tanh, ReLU, softmax, etc.\n",
    "-The activation function f affects the range and shape of the output values, and also influences the learning ability and speed of the network.\n",
    "\n",
    "Mathematically, forward propagation can be expressed as:\n",
    "\n",
    "y=f(Wx+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0609e08-ea31-4567-ad78-49ef4a69942c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee70765-6f03-4256-a47e-2f57aa877e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How are activation functions used during forward propagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14ccd5-e85f-4e36-9915-c46dc06cd8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e7b6f-0138-4440-b3fd-659962bd9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Activation functions are used to introduce non-linearity into the output of a neuron or a layer.\n",
    "\n",
    "-Activation functions also affect the ability and speed of a neural network to converge, or sometimes prevent it from converging.\n",
    "\n",
    "-Activation functions decide whether a neuron should be activated or not, and help to normalize the output of any input.\n",
    "\n",
    "There are many different activation functions, such as the sigmoid function, the tanh function, and the ReLU function. \n",
    "\n",
    "-The choice of activation function depends on the problem and the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67841a-a39f-47a5-8afb-058dd8744159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f76e2d-0769-4d31-890d-9dd57ee85066",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the role of weights and biases in forward propagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3eaf93-03ba-4fbd-acb9-acf82ead47fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c390a6f-37e0-4a02-b1ab-d92ea2d5ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Weights and biases are the parameters of a neural network that are learned during training.\n",
    "\n",
    "-Weights determine the strength of the connection between two neurons, while biases determine the threshold at which a neuron is activated.\n",
    "\n",
    "-Weights and biases affect how much input is passed between neurons and modulate the output and the input of each single neuron.\n",
    "\n",
    "-Weights and biases are updated using an optimization algorithm such as gradient descent, which minimizes a loss function that measures \n",
    " the difference between the predicted output and the actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2688aea-b302-431c-98fa-979118d0229d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5d9ea-7d14-4acf-892d-124ce84b893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67830a87-641d-44d8-9989-5bb448b4902e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c938dc-774e-435b-a443-7b699afc861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "-The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1.\n",
    "\n",
    "-The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability\n",
    " distribution.\n",
    "    \n",
    "-The softmax function is used for multi-class classification problems where class membership is required on more than two class labels.\n",
    "\n",
    "-The softmax function transforms the scores to a normalized probability distribution, which can be displayed to a user or used as input to\n",
    " other systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfeef8-f11a-45a7-8eed-07426771c6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d5be0-aeb7-4921-8ca6-d04abc564342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the purpose of backward propagation in a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce38522-4071-450e-a192-cbff7ff781ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f510c4b-c20b-4fb0-b033-c7dba8eebce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Backward propagation is the process of propagating the error from the output layer back to the input layer and updating the weights \n",
    " and biases accordingly.\n",
    "    \n",
    "-The purpose of backward propagation is to learn from the error and improve the performance of the network by minimizing the loss function.\n",
    "\n",
    "-Backward propagation involves calculating the gradients of the loss function with respect to each parameter using the chain rule\n",
    " and applying an optimization algorithm such as gradient descent to update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d481f4-3a79-47dc-99f2-174fb23c535b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c0f5e-a6b1-4ddb-980e-63251dfbdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a18425-2c7c-43a3-bc4d-1bb7a4850fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989ec32-1d96-48ed-9900-5d5f8b07b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Single-layer feedforward neural network\n",
    "\n",
    "-A single-layer feedforward neural network consists of an input layer and an output layer, with no hidden layers in between.\n",
    "\n",
    "-The input layer has one neuron for each feature or variable in the input data, and the output layer has one neuron for each class or \n",
    " category in the output data.\n",
    "    \n",
    "-The network learns a linear function that maps the input to the output, and applies an activation function to the output to introduce\n",
    " non-linearity and normalize the output values.\n",
    "    \n",
    "Backward propagation\n",
    "\n",
    "-Backward propagation is the process of propagating the error from the output layer back to the input layer and updating the weights \n",
    " and biases accordingly.\n",
    "    \n",
    "-The error is measured by a loss function that quantifies how well the network predicts the output data given the input data.\n",
    "\n",
    "-The weights and biases are updated using an optimization algorithm such as gradient descent, which minimizes the loss function by\n",
    " moving in the direction of the negative gradient.\n",
    "    \n",
    "-The gradient of the loss function with respect to each parameter is computed using the chain rule, which allows us to calculate \n",
    " the derivative of a composite function by multiplying the derivatives of each function that makes up the composite function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30352d-d31c-4f61-8134-0eeb1a8b33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mathematical formula\n",
    "\n",
    "Mathematically, backward propagation can be expressed as:\n",
    "\n",
    "∂W∂L​=∂y∂L​∂z∂y​∂W∂z​\n",
    "∂b∂L​=∂y∂L​∂z∂y​∂b∂z​\n",
    "\n",
    "Where:\n",
    "\n",
    "L is the loss function\n",
    "y is the output vector of size m×1\n",
    "z is the linear combination of input, weights and biases, i.e. z=Wx+b\n",
    "W is the weight matrix of size m×n\n",
    "b is the bias vector of size m×1\n",
    "∂y∂L​ is the gradient of the loss function with respect to the output vector, which depends on the type of loss function used\n",
    "∂z∂y​ is the gradient of the activation function with respect to the linear combination, which depends on the type of activation function used\n",
    "∂W∂z​ is the gradient of the linear combination with respect to the weight matrix, which is equal to the input vector transposed, i.e. ∂W∂z​=xT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132fe64-1846-41f7-a5b8-42cd148425ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620ca83-276e-4053-916d-6ec89514defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Can you explain the concept of the chain rule and its application in backward propagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed599e2-930b-4cd6-8746-ba81ffee7124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8c73e-03f9-4667-bf93-350dc231b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "-The chain rule is a mathematical rule that allows us to calculate the derivative of a composite function by multiplying the derivatives of \n",
    " each function that makes up the composite function.\n",
    "    \n",
    "-The chain rule is applied in backward propagation to calculate the gradients of the loss function with respect to each parameter by \n",
    " multiplying the gradients of each layer that makes up the network.\n",
    "    \n",
    "For example, if we have a function f(g(h(x))), then the derivative of f with respect to x is:\n",
    "\n",
    "dxdf=dgdfdh/dgdxdh\n",
    "\n",
    "Similarly, if we have a network with three layers, then the gradient of the loss function with respect to the weight matrix of the\n",
    "first layer is:\n",
    "\n",
    "∂W1​∂L​=∂y3​∂L​∂z3​∂y3​​∂y2​∂z3​​∂z2​∂y2​​∂y1​∂z2​​∂z1​∂y1​​∂W1​∂z1​​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e64be0-2409-4ae4-94ea-78c74e15abfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e17922-3b5d-48cd-b1a8-4944df5b2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What are some common challenges or issues that can occur during backward propagation, and how\n",
    "can they be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af84cd-098b-41ab-9532-49f7bad5376a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627e58b-0e25-4593-a094-5bc823ddf379",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some common challenges or issues that can occur during backward propagation are:\n",
    "    \n",
    "1.Vanishing gradients: This occurs when the gradients become very small as they are propagated back through many layers,\n",
    "  making it difficult for the network to learn and update its parameters. This can be addressed by using activation functions \n",
    "    that do not saturate easily, such as ReLU or leaky ReLU, or by using techniques such as batch normalization or residual\n",
    "    connections that help to maintain a stable gradient flow.\n",
    "    \n",
    "2.Exploding gradients: This occurs when the gradients become very large as they are propagated back through many layers, \n",
    "  causing numerical instability and overflow. This can be addressed by using gradient clipping or regularization techniques that\n",
    "    limit the magnitude of the gradients or penalize large weights.\n",
    "    \n",
    "3.Overfitting: This occurs when the network learns too well from the training data and fails to generalize to new or unseen data.\n",
    "  This can be addressed by using techniques such as dropout or early stopping that reduce the complexity or capacity of the network\n",
    "    or stop the training before it overfits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
